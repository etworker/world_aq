# 神经网络预测污染物数据需求分析

## 一、问题特点

### 数据特征
- **时序性**：污染物浓度有明显的日、周、季节性模式
- **多变量**：PM2.5、PM10、O3、NO2、SO2、CO 等多个污染物相互影响
- **多模态输入**：天气数据（温度、湿度、风速、气压等）+ 滞后特征 + 静态特征（城市、地理位置）
- **非线性关系**：污染物与天气之间存在复杂的非线性交互（如温度-湿度-PM2.5的U型关系）

### 当前模型局限
- Random Forest / Gradient Boosting：擅长处理非线性，但对长距离时序依赖建模较弱
- 滞后特征（lag features）：只能捕捉固定时间窗口的模式，无法学习动态依赖

---

## 二、适合的神经网络模型

### 1. LSTM / GRU（长短期记忆网络）

**适用场景**：捕捉长期时序依赖（如污染物的持续性、季节性变化）

**优势**：
- 天然适合序列数据，可处理任意长度的历史数据
- 门控机制（遗忘门、输入门、输出门）可选择性记忆/遗忘信息
- 对多变量时序建模效果好

**应用方式**：
- 输入：过去 N 小时/天的天气数据 + 污染物浓度
- 输出：未来 M 小时/天的污染物浓度

**参考案例**：Google DeepMind 的 GraphConvLSTM 用于 UK 空气质量预测

---

### 2. CNN + LSTM 混合模型

**适用场景**：同时捕捉空间模式（城市间扩散）和时序依赖

**优势**：
- CNN 提取局部空间特征（如区域污染扩散）
- LSTM 捕捉时间依赖
- 比纯 LSTM 更高效

**应用方式**：
- 将多个城市的污染物数据构建为"时空网格"
- CNN 提取空间特征 → LSTM 处理时序

---

### 3. Transformer-based 模型

**适用场景**：需要捕捉长距离依赖和注意力机制

**优势**：
- 自注意力机制可直接建模变量间的交互（如 PM2.5 与温度、湿度的关联）
- 并行计算，训练速度比 LSTM 快
- 可解释性强（注意力权重显示哪些历史时刻最重要）

**具体模型**：
- **Temporal Fusion Transformer (TFT)**：专为时间序列预测设计，支持多变量、静态协变量、可解释性
- **Informer**：针对长序列预测优化的 Transformer

**行业应用**：
- 亚马逊用 TFT 预测产品销量
- 谷歌用 Transformer 预测交通流量

---

### 4. Graph Neural Networks (GNN)

**适用场景**：考虑城市间的空间关联（如北京污染影响天津）

**优势**：
- 将城市建模为图节点，用边表示空间关联（距离、风向）
- 污染物扩散沿图传播，符合物理现实

**应用方式**：
- 图节点：各城市的天气 + 污染物数据
- 图边：基于距离或风向构建
- GNN + LSTM：同时处理空间和时间依赖

---

### 5. 深度集成模型

- **N-BEATS**：纯深度学习时序预测，无需特征工程
- **DeepAR**：Amazon 的概率时序预测，输出预测分布（而非单点值）
- **TCN (Temporal Convolutional Networks)**：用 1D 卷积捕捉时序模式，比 LSTM 更高效

---

## 三、与当前模型的对比

| 维度 | 当前模型 (RF/GBM) | 神经网络 (LSTM/Transformer) |
|------|------------------|------------------------------|
| **时序依赖** | 依赖手工滞后特征 | 自动学习长短期依赖 |
| **变量交互** | 树分裂自动捕捉 | 注意力机制显式建模 |
| **特征工程** | 需要大量特征设计 | 原始序列输入即可 |
| **数据需求** | 小样本效果好 | 需要大量数据（>10k样本） |
| **训练时间** | 秒级 | 分钟-小时级 |
| **可解释性** | 特征重要性直观 | 黑盒，需额外分析 |
| **部署复杂度** | 简单（joblib） | 需要推理框架 |

---

## 四、数据量需求分析

### 1. 基于模型参数的经验公式

**传统机器学习（RF/GBM）**：
- 样本数 > 特征数 × 10
- 当前：~20 特征 × 10 = 200 样本即可

**神经网络**：
- 样本数 > 模型参数量 × 5-10
- 简单 LSTM（2层，64 hidden）：
  - 参数量 ≈ 输入维度 × 64 × 2 + 64 × 64 × 2 + 64 × 1 ≈ 5,000-10,000
  - 需要样本数：50,000 - 100,000

**深度模型（TFT/Transformer）**：
- 参数量：100,000 - 1,000,000
- 需要样本数：1,000,000+

### 2. 时序数据特有规则

**时间窗口 vs 样本数**：
- 如果预测未来 1 天，用过去 7 天数据作为输入
- 每个城市每天可生成 1 个样本
- 样本数 = 城市数 × 年份 × 365 - 时间窗口损失

**示例计算**：
```
当前数据：
- 6 城市 × 3 年 × 365天 = 6,570 样本

目标数据：
- 50 城市 × 5 年 × 365天 = 91,250 样本  ← LSTM 起步
- 100 城市 × 10 年 × 365天 = 365,000 样本  ← LSTM 稳定
- 200 城市 × 15 年 × 365天 = 1,095,000 样本  ← 深度模型起步
```

---

## 五、分阶段数据需求建议

### 阶段1：LSTM 轻量级模型（推荐起点）

**目标**：验证神经网络是否优于 RF/GBM

**数据需求**：
- 城市数：30-50 个
- 年份：5-8 年
- 总样本：50,000 - 100,000

**模型配置**：
- 2层 BiLSTM，hidden=64
- Dropout=0.3
- 参数量：~5,000

**验证指标**：
- 验证集 RMSE 低于 RF/GBM
- 不严重过拟合（验证损失稳定）

---

### 阶段2：TFT 进阶模型

**目标**：提升预测性能 + 可解释性

**数据需求**：
- 城市数：50-100 个
- 年份：8-12 年
- 总样本：150,000 - 400,000

**模型配置**：
- 4层 Transformer Encoder，heads=4
- 参数量：~50,000

**验证指标**：
- RMSE 比 LSTM 提升 >5%
- 注意力权重可解释（显示重要特征）

---

### 阶段3：深度模型（GNN + LSTM）

**目标**：建模空间扩散 + 复杂时序依赖

**数据需求**：
- 城市数：100-200 个（需要空间网络）
- 年份：10-15 年
- 总样本：500,000 - 1,000,000

**模型配置**：
- GNN (GraphConv) + 3层 LSTM
- 参数量：~100,000

**验证指标**：
- 显著优于单城市模型
- 捕捉城市间污染扩散

---

## 六、城市数量 vs 数据年份的权衡

| 策略 | 优势 | 劣势 | 适用场景 |
|------|------|------|----------|
| **多城市 + 少年份** | 捕捉空间异质性，提升泛化能力 | 每个城市数据少，时序模式学习不充分 | 城市气候差异大（如北京 vs 洛杉矶） |
| **少城市 + 多年份** | 时序模式学习充分 | 空间泛化能力弱，过拟合风险 | 单一气候区研究（如京津冀） |
| **平衡策略** | 兼顾时空泛化 | 数据收集成本高 | 实际应用推荐 |

**推荐策略**：
```
优先增加城市数量（空间多样性），其次是年份长度

原因：
1. 不同城市的气候、工业结构差异大，增加城市能学到更通用的模式
2. 气候变化导致历史数据分布漂移，过于久远的数据可能失效
3. 新城市可直接迁移学习，无需重新训练
```

---

## 七、数据质量要求

### 1. 数据完整性
```
理想：缺失率 < 5%
可接受：缺失率 < 15%（需插补）
不可用：缺失率 > 30%
```

### 2. 数据平衡性
- 各城市数据量均衡（避免某些城市主导训练）
- 各季节数据量均衡（避免季节偏斜）

### 3. 数据覆盖性
- 覆盖极端事件（如野火、沙尘暴）
- 覆盖政策变化（如排放标准调整）

---

## 八、实际案例参考

| 模型 | 城市数 | 年份 | 样本数 | 数据来源 |
|------|--------|------|--------|----------|
| Google GraphConvLSTM | 50 (UK) | 4 | ~73,000 | UK Air Quality |
| Microsoft TFT | 20 (中国) | 6 | ~43,800 | 自研 |
| Baidu DeepAR | 100+ | 10+ | ~365,000+ | 公开数据集 |
| 清华 CNN-LSTM | 13 (京津冀) | 5 | ~23,725 | 中国环境监测 |

---

## 九、稳定性评估指标

### 训练稳定性
1. **损失曲线**：平滑下降，无剧烈震荡
2. **验证损失**：训练损失稳定时，验证损失不上升
3. **多轮训练**：不同随机种子下性能波动 < 5%

### 预测稳定性
1. **RMSE**：验证集 RMSE < 测试集 RMSE × 1.2
2. **R²**：验证集 R² > 0.8
3. **一致性**：相似输入的预测值差异 < 10%

---

## 十、渐进式数据扩充路线图

### 第1步：当前基线（6城市 × 3年）
```
样本数：6,570
模型：Random Forest
用途：验证特征工程有效性
```

### 第2步：小规模扩充（30城市 × 5年）
```
样本数：54,750
模型：2层 LSTM (hidden=64)
预期：RMSE 相比 RF 提升 5-10%
里程碑：验证神经网络可行性
```

### 第3步：中等规模（80城市 × 10年）
```
样本数：292,000
模型：TFT (4层, 4 heads)
预期：RMSE 再提升 10-15%
里程碑：模型达到生产可用水平
```

### 第4步：大规模（150城市 × 15年）
```
样本数：821,250
模型：GNN + LSTM
预期：RMSE 再提升 5-10%，支持空间扩散预测
里程碑：支持全国范围预测
```

---

## 十一、实际建议

### 最低可行配置（MVP）
```
城市数：40-50 个
年份：6-8 年
总样本：90,000 - 140,000

理由：
- 足够训练 2层 LSTM
- 覆盖主要气候类型
- 数据收集成本可控

预期：
- LSTM 性能 ≈ RF/GBM
- 训练时间：10-30分钟
- 硬件要求：单张 GPU 或高性能 CPU
```

### 推荐配置（生产级）
```
城市数：100 个
年份：10 年
总样本：365,000

理由：
- 支持复杂模型（TFT）
- 捕捉长期气候周期
- 跨城市泛化能力强

预期：
- RMSE 比 RF 提升 15-25%
- 可解释性强（注意力机制）
- 训练时间：2-4小时（GPU）
```

### 理想配置（研究级）
```
城市数：200+ 个
年份：15 年
总样本：1,000,000+

理由：
- 支持最先进模型（GNN + LSTM）
- 研究污染扩散机制
- 支持极端事件预测

预期：
- 最优预测性能
- 科学研究价值高
- 训练时间：1天（多GPU）
```

---

## 十二、数据收集优先级

### 高优先级城市（按污染严重度和数据质量排序）
1. 京津冀城市群（北京、天津、石家庄等）
2. 长三角城市群（上海、南京、杭州等）
3. 珠三角城市群（广州、深圳、佛山等）
4. 中西部重工业城市（西安、成都、武汉等）
5. 东北工业城市（沈阳、哈尔滨等）

### 高优先级年份
- 最近 5 年（数据质量好、传感器密度高）
- 包含极端事件的年份（如 2020年澳洲山火影响全球）

---

## 十三、潜在挑战

### 1. 过拟合风险
- 当前数据规模下，NN 可能不如 RF/GBM 稳健

### 2. 计算资源
- 训练时间更长（分钟级 vs 秒级）

### 3. 超参数调优
- 网络结构、学习率、batch size 等需要大量实验

### 4. 可解释性
- 神经网络是黑盒，难以向决策者解释预测依据

### 5. 数据质量
- 传感器数据缺失需要专门的插补策略（NN 对缺失值敏感）

---

## 十四、输入设计建议

```
输入维度：[batch_size, time_steps, features]

特征：
- 时间序列特征：过去 7 天 × 每日污染物 + 天气
- 滞后特征：PM2.5 lag-1, lag-2, ..., lag-7
- 天气特征：温度、湿度、风速、气压等
- 静态特征：城市、季节（embedding）
```

---

## 十五、模型架构推荐

### 轻量级方案（适合当前数据规模）
```
输入 → Embedding(静态特征) → 拼接 → 
2层 BiLSTM (hidden=64) → Dropout(0.3) → 
Dense(32) → Output(PM2.5预测)
```

### 进阶方案（数据充足时）
```
输入 → Transformer Encoder (4 heads, 4 layers) → 
TFT 解码器 → 输出(多污染物预测)
```

---

## 十六、训练策略

### 损失函数
- MSE + 对数变换损失（解决目标分布偏斜）

### 优化器
- AdamW（带权重衰减）

### 学习率调度
- Cosine Annealing + Warmup

### 早停
- 监控验证集 RMSE

---

## 十七、结论

### 最小数据量
- 30城市 × 6年 = 65,700 样本 → 可训练简单 LSTM

### 稳定训练数据量
- 100城市 × 10年 = 365,000 样本 → 可训练 TFT，性能稳定

### 最优数据量
- 200城市 × 15年 = 1,095,000 样本 → 可训练深度模型，科研级

### 策略建议
1. 先用 50城市 × 8年 验证神经网络可行性
2. 如有效，扩充至 100城市 × 10年 达到生产级
3. 最后扩充至 200+城市 × 15年 进行深度研究

### 核心原则
优先增加城市数量（空间多样性），其次是年份长度（时序深度）。